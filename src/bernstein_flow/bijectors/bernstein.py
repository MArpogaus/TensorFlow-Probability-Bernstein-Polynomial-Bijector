#!env python3
# AUTHOR INFORMATION ##########################################################
# file    : bernstein_bijector.py
# brief   : [Description]
#
# author  : Marcel Arpogaus
# created : 2020-09-11 14:14:24
# changed : 2020-12-07 16:29:11
# DESCRIPTION #################################################################
#
# This project is following the PEP8 style guide:
#
#    https://www.python.org/dev/peps/pep-0008/)
#
# COPYRIGHT ###################################################################
# Copyright 2020 Marcel Arpogaus
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
###############################################################################

# REQUIRED PYTHON MODULES #####################################################
import scipy.interpolate as interp

import numpy as np
import tensorflow as tf

from tensorflow_probability import distributions as tfd
from tensorflow_probability import bijectors as tfb

from tensorflow_probability.python.internal import dtype_util
from tensorflow_probability.python.internal import tensor_util
from tensorflow_probability.python.internal import tensorshape_util
from tensorflow_probability.python.internal import prefer_static


def constrain_thetas(
    thetas_unconstrained: tf.Tensor,
    high=tf.constant(3.0, name="high"),
    low=tf.constant(-3.0, name="low"),
    allow_values_outside_support=False,
    eps=1e-4,
    fn=tf.math.softmax,
) -> tf.Tensor:
    """Ensures monotone increasing Bernstein coefficients.

    :param thetas_unconstrained: Tensor containing the distance of the
      Bernstein coefficients.
    :param low: The lower bound.
    :param high: The upper bound.
    :param allow_values_outside_support: Use the first and last parameter to
      increase/decrease the upper/lower bound.
    :param eps: Optional minimum distance of thetas. Default Value: 1e-8
    :param fn: Function to ensure positive values.
    :returns:   Moncton increasing Bernstein coefficients.
    :rtype:     Tensor

    """
    with tf.name_scope("constrain_theta"):
        if allow_values_outside_support:
            low -= fn(thetas_unconstrained[..., :1], name="low")
            high += fn(thetas_unconstrained[..., -1:], name="high")
            d = fn(thetas_unconstrained[..., 1:-1]) + eps
        else:
            d = fn(thetas_unconstrained) + eps
        d /= tf.reduce_sum(d, axis=-1)[..., None]
        d *= high - low
        tc = tf.concat(
            (
                low * tf.ones_like(thetas_unconstrained[..., :1]),
                d,
            ),
            axis=-1,
        )
        return tf.cumsum(tc, axis=-1, name="theta")


class BernsteinBijector(tfb.Bijector):
    """
    Implementing Bernstein polynomials using the `tfb.Bijector` interface for
    transformations of a `Distribution` sample.
    """

    def __init__(
        self,
        thetas: tf.Tensor,
        validate_args: bool = False,
        name: str = "bernstein_bijector",
    ):
        """
        Constructs a new instance of a Bernstein polynomial bijector.

        :param      theta:          The Bernstein coefficients.
        :type       theta:          Tensor
        :param      validate_args:  Whether to validate input with asserts.
                                    Passed to `super()`.
        :type       validate_args:  bool
        :param      name:           The name to give Ops created by the
                                    initializer. Passed to `super()`.
        :type       name:           str
        """
        with tf.name_scope(name) as name:
            dtype = dtype_util.common_dtype([thetas], dtype_hint=tf.float32)

            self.thetas = tensor_util.convert_nonref_to_tensor(thetas, dtype=dtype)

            shape = prefer_static.shape(self.thetas)
            self.order = shape[-1]
            self.batch_shape = shape[:-1]

            # Bernstein polynomials of order M,
            # generated by the M + 1 beta-densities
            self.beta_dist_h = tfd.Beta(
                tf.range(1, self.order + 1, dtype=tf.float32),
                tf.range(self.order, 0, -1, dtype=tf.float32),
            )

            # Deviation of the Bernstein polynomials
            self.beta_dist_h_dash = tfd.Beta(
                tf.range(1, self.order, dtype=tf.float32),
                tf.range(self.order - 1, 0, -1, dtype=tf.float32),
            )

            # Cubic splines are used to approximate the inverse
            self.interp = None

            super().__init__(
                forward_min_event_ndims=0,
                validate_args=validate_args,
                dtype=dtype,
                name=name,
            )

    def gen_inverse_interpolation(self) -> None:
        """
        Generates the Spline Interpolation.
        """
        n_points = 200
        rank = tensorshape_util.rank(self.batch_shape)
        shape = [...] + [tf.newaxis] * rank

        clip = 1e-7
        y_fit = np.linspace(clip, 1 - clip, n_points, dtype=np.float32)

        z_fit = self.forward(y_fit[tuple(shape)])
        z_fit = z_fit.numpy().reshape(n_points, -1)

        self.z_min_interp = np.min(z_fit, axis=0)
        self.z_max_interp = np.max(z_fit, axis=0)

        ips = [
            interp.interp1d(
                x=np.squeeze(z_fit[..., i]),
                y=np.squeeze(y_fit),
                kind="cubic",
                # bc_type='natural',
                assume_sorted=True,
            )
            for i in range(z_fit.shape[-1])
        ]

        def ifn(z):
            y = []
            z_clip = np.clip(z, self.z_min_interp, self.z_max_interp)
            for i, ip in enumerate(ips):
                y.append(ip(z_clip[..., i]).astype(np.float32))
            y = np.stack(y, axis=-1)
            return y

        self.interp = ifn

    def reshape_out(self, sample_shape, y):
        output_shape = prefer_static.broadcast_shape(sample_shape, self.batch_shape)
        return tf.reshape(y, output_shape)

    def _inverse(self, z: tf.Tensor) -> tf.Tensor:
        """
        Returns the inverse Bijector evaluation.

        :param      z:    The input to the inverse evaluation.
        :type       z:    Tensor

        :returns:   The inverse Bijector evaluation.
        :rtype:     Tensor
        """
        if tf.executing_eagerly():
            batch_rank = tensorshape_util.rank(self.batch_shape)
            sample_shape = z.shape

            if sample_shape[-batch_rank:] == self.batch_shape:
                shape = list(sample_shape[:-batch_rank]) + [-1]
                z = tf.reshape(z, shape)
            else:
                z = z[..., None]

            if self.interp is None:
                self.gen_inverse_interpolation()
            y = self.reshape_out(sample_shape, self.interp(z))
        else:
            y = z

        return y

    def _forward(self, y: tf.Tensor) -> tf.Tensor:
        """
        Returns the forward Bijector evaluation.

        :param      y:    The input to the forward evaluation.
        :type       y:    Tensor

        :returns:   The forward Bijector evaluation.
        :rtype:     Tensor
        """
        sample_shape = prefer_static.shape(y)
        y = y[..., tf.newaxis]

        y = tf.clip_by_value(y, 0, 1.0)
        by = self.beta_dist_h.prob(y)
        z = tf.reduce_mean(by * self.thetas, axis=-1)

        return self.reshape_out(sample_shape, z)

    def _forward_log_det_jacobian(self, y):
        sample_shape = prefer_static.shape(y)
        y = y[..., tf.newaxis]

        y = tf.clip_by_value(y, 0, 1.0)
        by = self.beta_dist_h_dash.prob(y)
        dtheta = self.thetas[..., 1:] - self.thetas[..., 0:-1]
        ldj = tf.math.log(tf.reduce_sum(by * dtheta, axis=-1))

        return self.reshape_out(sample_shape, ldj)

    @classmethod
    def constrain_theta(cls: type, *args, **kwds) -> tf.Tensor:
        return constrain_thetas(*args, **kwds)

    def _is_increasing(self, **kwargs):
        return tf.reduce_all(self.thetas[..., 1:] >= self.thetas[..., :-1])
